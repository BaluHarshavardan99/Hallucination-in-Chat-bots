* Hallucination in Chat-bots *

This repository conatins code for the final course project - "Faithful Benchmark for Information-Seeking Dialogue - Fact Hallucinations Detection and Prevention" and detailed instructions to run the code

There are two folders present in this repository - milestone-2 and and milestone-3.

Milestone-2 folder contains our baseline model code which is RoBERTa-large model.

Milestone-3 folder contains our final model code which is a modified version of RoBERTa-large model.


Abstract: <br>
Chatbots can provide a convenient and efficient means of communication, but they are not immune to errors. Hallucination is one of the most significant problems associated with chatbots. When a chatbot provides factually incorrect responses, it can be detrimental to the user and the chatbot's performance. Hallucinations can result in users receiving misinformation or even harm. To address this issue, we propose a model to detect and prevent hallucinations in Information-Seeking Dialogue-based NLP systems. In this project, we explore various techniques to identify and reduce hallucinations in NLG models. Our approach involves combining rule-based techniques and machine learning algorithms to detect potential hallucinations in the model's output. Once identified, we prevent the model from generating such responses in the future by modifying the training data, reweighting model parameters, or introducing a feedback loop with human experts. Our project aims to enhance user trust, improve the overall quality of chatbots, and reduce the potential harm caused by misinformation.
